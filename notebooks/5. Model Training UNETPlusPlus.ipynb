{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\n",
    "\n",
    "for x in [input_images, target_masks]:\n",
    "    print(x.shape)\n",
    "    print(x.min(), x.max())\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image, Right: Target mask (Ground-truth)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 2000, 'val': 200}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count, transform=None):\n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(192, 192, count=count)        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_images)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return [image, mask]\n",
    "\n",
    "# use same transform for train/val for this example\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = SimDataset(2000, transform = trans)\n",
    "val_set = SimDataset(200, transform = trans)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "\n",
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-15          [-1, 256, 28, 28]               0\n",
      "           Conv2d-16          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-17          [-1, 512, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "         Upsample-20          [-1, 512, 56, 56]               0\n",
      "           Conv2d-21          [-1, 256, 56, 56]       1,769,728\n",
      "             ReLU-22          [-1, 256, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-24          [-1, 256, 56, 56]               0\n",
      "         Upsample-25        [-1, 256, 112, 112]               0\n",
      "           Conv2d-26        [-1, 128, 112, 112]         442,496\n",
      "             ReLU-27        [-1, 128, 112, 112]               0\n",
      "           Conv2d-28        [-1, 128, 112, 112]         147,584\n",
      "             ReLU-29        [-1, 128, 112, 112]               0\n",
      "         Upsample-30        [-1, 128, 224, 224]               0\n",
      "           Conv2d-31         [-1, 64, 224, 224]         110,656\n",
      "             ReLU-32         [-1, 64, 224, 224]               0\n",
      "           Conv2d-33         [-1, 64, 224, 224]          36,928\n",
      "             ReLU-34         [-1, 64, 224, 224]               0\n",
      "           Conv2d-35          [-1, 6, 224, 224]             390\n",
      "================================================================\n",
      "Total params: 7,783,238\n",
      "Trainable params: 7,783,238\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = pytorch_unet.UNet(6)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.210124, dice: 0.994346, loss: 0.602235\n",
      "val: bce: 0.030143, dice: 0.986439, loss: 0.508291\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 1/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.022030, dice: 0.806168, loss: 0.414099\n",
      "val: bce: 0.023499, dice: 0.671528, loss: 0.347514\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 2/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.023134, dice: 0.522101, loss: 0.272618\n",
      "val: bce: 0.017994, dice: 0.439513, loss: 0.228753\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 3/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.015791, dice: 0.392756, loss: 0.204273\n",
      "val: bce: 0.015154, dice: 0.353304, loss: 0.184229\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 4/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.012854, dice: 0.299000, loss: 0.155927\n",
      "val: bce: 0.011838, dice: 0.235490, loss: 0.123664\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 5/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.010764, dice: 0.217516, loss: 0.114140\n",
      "val: bce: 0.010928, dice: 0.202027, loss: 0.106478\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 6/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.010902, dice: 0.222725, loss: 0.116813\n",
      "val: bce: 0.010661, dice: 0.192998, loss: 0.101830\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 7/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.009604, dice: 0.184641, loss: 0.097122\n",
      "val: bce: 0.010067, dice: 0.181135, loss: 0.095601\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 8/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.009128, dice: 0.176201, loss: 0.092664\n",
      "val: bce: 0.008653, dice: 0.176254, loss: 0.092453\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 9/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.008457, dice: 0.170643, loss: 0.089550\n",
      "val: bce: 0.008299, dice: 0.171656, loss: 0.089977\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 10/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.007046, dice: 0.151076, loss: 0.079061\n",
      "val: bce: 0.005749, dice: 0.138535, loss: 0.072142\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 11/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.004789, dice: 0.094846, loss: 0.049817\n",
      "val: bce: 0.004794, dice: 0.082758, loss: 0.043776\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 12/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003822, dice: 0.066693, loss: 0.035258\n",
      "val: bce: 0.004868, dice: 0.075574, loss: 0.040221\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 13/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003647, dice: 0.065981, loss: 0.034814\n",
      "val: bce: 0.005102, dice: 0.078447, loss: 0.041774\n",
      "0m 43s\n",
      "Epoch 14/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003680, dice: 0.068849, loss: 0.036265\n",
      "val: bce: 0.004177, dice: 0.066650, loss: 0.035413\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 15/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.003029, dice: 0.053153, loss: 0.028091\n",
      "val: bce: 0.003654, dice: 0.061158, loss: 0.032406\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 16/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002797, dice: 0.050167, loss: 0.026482\n",
      "val: bce: 0.003610, dice: 0.059508, loss: 0.031559\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 17/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002720, dice: 0.049958, loss: 0.026339\n",
      "val: bce: 0.003184, dice: 0.057431, loss: 0.030307\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 18/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002537, dice: 0.046737, loss: 0.024637\n",
      "val: bce: 0.003113, dice: 0.054996, loss: 0.029055\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 19/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002300, dice: 0.044468, loss: 0.023384\n",
      "val: bce: 0.002945, dice: 0.051255, loss: 0.027100\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 20/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.002042, dice: 0.040555, loss: 0.021299\n",
      "val: bce: 0.002866, dice: 0.050504, loss: 0.026685\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 21/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001988, dice: 0.038980, loss: 0.020484\n",
      "val: bce: 0.002593, dice: 0.047394, loss: 0.024993\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 22/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001841, dice: 0.036638, loss: 0.019239\n",
      "val: bce: 0.002522, dice: 0.045939, loss: 0.024230\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 23/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001795, dice: 0.035693, loss: 0.018744\n",
      "val: bce: 0.002727, dice: 0.044743, loss: 0.023735\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 24/39\n",
      "----------\n",
      "LR 0.0001\n",
      "train: bce: 0.001691, dice: 0.034025, loss: 0.017858\n",
      "val: bce: 0.002360, dice: 0.043020, loss: 0.022690\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 25/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001572, dice: 0.031303, loss: 0.016437\n",
      "val: bce: 0.002217, dice: 0.040832, loss: 0.021524\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 26/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001514, dice: 0.030473, loss: 0.015993\n",
      "val: bce: 0.002166, dice: 0.040488, loss: 0.021327\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 27/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001501, dice: 0.030128, loss: 0.015815\n",
      "val: bce: 0.002229, dice: 0.040340, loss: 0.021285\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 28/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001496, dice: 0.029890, loss: 0.015693\n",
      "val: bce: 0.002166, dice: 0.040157, loss: 0.021162\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 29/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001488, dice: 0.029740, loss: 0.015614\n",
      "val: bce: 0.002215, dice: 0.040059, loss: 0.021137\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 30/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001479, dice: 0.029537, loss: 0.015508\n",
      "val: bce: 0.002149, dice: 0.039748, loss: 0.020948\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 31/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001469, dice: 0.029364, loss: 0.015416\n",
      "val: bce: 0.002212, dice: 0.039819, loss: 0.021016\n",
      "0m 43s\n",
      "Epoch 32/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001470, dice: 0.029170, loss: 0.015320\n",
      "val: bce: 0.002146, dice: 0.039689, loss: 0.020918\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 33/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001456, dice: 0.029055, loss: 0.015255\n",
      "val: bce: 0.002180, dice: 0.039492, loss: 0.020836\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 34/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001451, dice: 0.028900, loss: 0.015175\n",
      "val: bce: 0.002170, dice: 0.039412, loss: 0.020791\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 35/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001432, dice: 0.028700, loss: 0.015066\n",
      "val: bce: 0.002203, dice: 0.039768, loss: 0.020985\n",
      "0m 43s\n",
      "Epoch 36/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001433, dice: 0.028581, loss: 0.015007\n",
      "val: bce: 0.002091, dice: 0.039245, loss: 0.020668\n",
      "saving best model\n",
      "0m 43s\n",
      "Epoch 37/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001422, dice: 0.028358, loss: 0.014890\n",
      "val: bce: 0.002160, dice: 0.039272, loss: 0.020716\n",
      "0m 43s\n",
      "Epoch 38/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001414, dice: 0.028230, loss: 0.014822\n",
      "val: bce: 0.002143, dice: 0.039213, loss: 0.020678\n",
      "0m 43s\n",
      "Epoch 39/39\n",
      "----------\n",
      "LR 1e-05\n",
      "train: bce: 0.001406, dice: 0.027994, loss: 0.014700\n",
      "val: bce: 0.002083, dice: 0.039034, loss: 0.020559\n",
      "saving best model\n",
      "0m 43s\n",
      "Best val loss: 0.020559\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 6\n",
    "\n",
    "model = pytorch_unet.UNet(num_class).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e0354882044411ee0ea7a57c0ce12312a12a0c6d66013e45a91a66bf338708a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
