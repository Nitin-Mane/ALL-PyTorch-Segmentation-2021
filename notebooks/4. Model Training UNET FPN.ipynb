{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%matplotlib inline\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2\r\n",
    "\r\n",
    "import os,sys\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import helper\r\n",
    "import simulation\r\n",
    "\r\n",
    "# Generate some random images\r\n",
    "input_images, target_masks = simulation.generate_random_data(192, 192, count=3)\r\n",
    "\r\n",
    "print(input_images.shape, target_masks.shape)\r\n",
    "\r\n",
    "# Change channel-order and make 3 channels for matplot\r\n",
    "input_images_rgb = [(x.swapaxes(0, 2).swapaxes(0,1) * -255 + 255).astype(np.uint8) for x in input_images]\r\n",
    "\r\n",
    "# Map each channel (i.e. class) to each color\r\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\r\n",
    "\r\n",
    "# Left: Input image, Right: Target mask\r\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "from torchvision import models\r\n",
    "\r\n",
    "base_model = models.resnet18(pretrained=True)\r\n",
    "\r\n",
    "def find_last_layer(layer):\r\n",
    "    children = list(layer.children())\r\n",
    "    if len(children) == 0:\r\n",
    "        return layer\r\n",
    "    else:\r\n",
    "        return find_last_layer(children[-1])\r\n",
    "    \r\n",
    "list(base_model.children())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "from torch import nn\r\n",
    "\r\n",
    "model_wo_avgpool = nn.Sequential(*list(base_model.children())[:-2])\r\n",
    "\r\n",
    "#OrderedDict(model_wo_avgpool.named_children())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'OrderedDict' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e31efe363266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_wo_avgpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wo_avgpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'OrderedDict' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "import torch\r\n",
    "\r\n",
    "class FCN(nn.Module):\r\n",
    "\r\n",
    "    def __init__(self, n_class):\r\n",
    "        super().__init__()\r\n",
    "        \r\n",
    "        self.base_model = models.resnet18(pretrained=True)\r\n",
    "        \r\n",
    "        layers = list(base_model.children())\r\n",
    "        self.layer1 = nn.Sequential(*layers[:5]) # size=(N, 64, x.H/2, x.W/2)\r\n",
    "        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear')\r\n",
    "        self.layer2 = layers[5]  # size=(N, 128, x.H/4, x.W/4)\r\n",
    "        self.upsample2 = nn.Upsample(scale_factor=8, mode='bilinear')\r\n",
    "        self.layer3 = layers[6]  # size=(N, 256, x.H/8, x.W/8)\r\n",
    "        self.upsample3 = nn.Upsample(scale_factor=16, mode='bilinear')\r\n",
    "        self.layer4 = layers[7]  # size=(N, 512, x.H/16, x.W/16)\r\n",
    "        self.upsample4 = nn.Upsample(scale_factor=32, mode='bilinear')\r\n",
    "        \r\n",
    "        self.conv1k = nn.Conv2d(64 + 128 + 256 + 512, n_class, 1)\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.layer1(x)\r\n",
    "        up1 = self.upsample1(x)\r\n",
    "        x = self.layer2(x)\r\n",
    "        up2 = self.upsample2(x)\r\n",
    "        x = self.layer3(x)\r\n",
    "        up3 = self.upsample3(x)\r\n",
    "        x = self.layer4(x)\r\n",
    "        up4 = self.upsample4(x)\r\n",
    "        \r\n",
    "        merge = torch.cat([up1, up2, up3, up4], dim=1)\r\n",
    "        merge = self.conv1k(merge)\r\n",
    "        out = self.sigmoid(merge)\r\n",
    "        \r\n",
    "        return out\r\n",
    "\r\n",
    "fcn_model = FCN(6)\r\n",
    "\r\n",
    "import torchsummary\r\n",
    "\r\n",
    "#torchsummary.summary(fcn_model, input_size=(3, 224, 224), device='cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\r\n",
    "    since = time.time()\r\n",
    "\r\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "    best_loss = 1e10\r\n",
    "\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\r\n",
    "        print('-' * 10)\r\n",
    "\r\n",
    "        # Each epoch has a training and validation phase\r\n",
    "        for phase in ['train', 'val']:\r\n",
    "            if phase == 'train':\r\n",
    "                scheduler.step()\r\n",
    "                model.train()  # Set model to training mode\r\n",
    "            else:\r\n",
    "                model.eval()   # Set model to evaluate mode\r\n",
    "\r\n",
    "            running_loss = 0.0\r\n",
    "            running_corrects = 0\r\n",
    "\r\n",
    "            # Iterate over data.\r\n",
    "            batch_size = 10\r\n",
    "            epoch_steps = 10\r\n",
    "            for i in range(epoch_steps):\r\n",
    "                input_images, target_masks = simulation.generate_random_data(192, 192, count=batch_size)\r\n",
    "\r\n",
    "                inputs = torch.from_numpy(input_images)\r\n",
    "                labels = torch.from_numpy(target_masks)\r\n",
    "                inputs = inputs.to(device)\r\n",
    "                labels = labels.to(device)                \r\n",
    "\r\n",
    "                # zero the parameter gradients\r\n",
    "                optimizer.zero_grad()\r\n",
    "\r\n",
    "                # forward\r\n",
    "                # track history if only in train\r\n",
    "                with torch.set_grad_enabled(phase == 'train'):\r\n",
    "                    outputs = model(inputs)\r\n",
    "                    loss = criterion(outputs, labels)\r\n",
    "\r\n",
    "                    # backward + optimize only if in training phase\r\n",
    "                    if phase == 'train':\r\n",
    "                        loss.backward()\r\n",
    "                        optimizer.step()\r\n",
    "\r\n",
    "                # statistics\r\n",
    "                running_loss += loss.item() * inputs.size(0)\r\n",
    "\r\n",
    "            epoch_loss = running_loss / (batch_size * epoch_steps)\r\n",
    "\r\n",
    "            print('{} Loss: {:.4f}'.format(\r\n",
    "                phase, epoch_loss))\r\n",
    "\r\n",
    "            # deep copy the model\r\n",
    "            if phase == 'val' and epoch_loss < best_loss:\r\n",
    "                best_loss = epoch_loss\r\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\r\n",
    "\r\n",
    "        print()\r\n",
    "\r\n",
    "    time_elapsed = time.time() - since\r\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\r\n",
    "        time_elapsed // 60, time_elapsed % 60))\r\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\r\n",
    "\r\n",
    "    # load best model weights\r\n",
    "    model.load_state_dict(best_model_wts)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "from torch.optim import lr_scheduler\r\n",
    "import time\r\n",
    "import copy\r\n",
    "\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "model_ft = FCN(6).to(device)\r\n",
    "\r\n",
    "criterion = nn.BCELoss()\r\n",
    "\r\n",
    "# Observe that all parameters are being optimized\r\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\r\n",
    "\r\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\r\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\r\n",
    "\r\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train Loss: 0.4876\n",
      "val Loss: 0.2639\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1647\n",
      "val Loss: 0.0984\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.0781\n",
      "val Loss: 0.0628\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.0570\n",
      "val Loss: 0.0539\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.0469\n",
      "val Loss: 0.0464\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.0443\n",
      "val Loss: 0.0441\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.0420\n",
      "val Loss: 0.0425\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0416\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0407\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0411\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0405\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0402\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0406\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0402\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0405\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0411\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0406\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0410\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0396\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0403\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0410\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0402\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0393\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0405\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0409\n",
      "val Loss: 0.0410\n",
      "\n",
      "Training complete in 1m 11s\n",
      "Best val loss: 0.039217\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "6e0354882044411ee0ea7a57c0ce12312a12a0c6d66013e45a91a66bf338708a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}